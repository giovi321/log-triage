{
  "root": "Configuration sections: llm, rag, pipelines, modules, database, webui, and logging. Each section contains detailed options for log processing, AI analysis, and system behavior.",
  "llm": "llm: global LLM defaults and provider definitions shared by every module. Configure OpenAI-compatible endpoints, sampling parameters, and context limits.",
  "llm_enabled": "llm.enabled: master switch that allows modules to call an OpenAI-compatible provider when a finding needs LLM analysis.",
  "llm_default_provider": "llm.default_provider: name of the provider to use when modules do not specify one; if omitted and only one provider exists it is picked automatically.",
  "llm_summary_prompt_path": "llm.summary_prompt_path: filesystem path to the prompt template used for generating high-level summaries of multiple findings.",
  "llm_context_prefix_lines": "llm.context_prefix_lines: global default for number of lines to prepend before a matched finding line when building the excerpt for the LLM.",
  "llm_context_suffix_lines": "llm.context_suffix_lines: global default for number of lines to append after a matched finding line when building the excerpt for the LLM.",
  "llm_providers": "llm.providers.<name>: OpenAI/vLLM compatible endpoint definition with api_base, api_key_env, model, and optional organization/api_version/request_timeout/temperature/top_p/max_output_tokens/max_excerpt_lines. See OpenAI's parameter guide for how sampling works: https://platform.openai.com/docs/guides/text-generation/parameter-details.",
  "llm_providers_*": "llm.providers.<name>: OpenAI/vLLM compatible endpoint definition. Use api_base, api_key_env, model, and optional parameters.",
  "llm_providers_api_base": "llm.providers.<name>.api_base: base URL of the OpenAI-compatible endpoint (e.g., https://api.openai.com/v1).",
  "llm_providers_*_api_base": "llm.providers.<name>.api_base: base URL of the OpenAI-compatible endpoint (e.g., https://api.openai.com/v1).",
  "llm_providers_api_key_env": "llm.providers.<name>.api_key_env: environment variable that holds the API key used for this provider.",
  "llm_providers_*_api_key_env": "llm.providers.<name>.api_key_env: environment variable that holds the API key used for this provider.",
  "llm_providers_model": "llm.providers.<name>.model: default model name used for completions unless overridden by a module.",
  "llm_providers_*_model": "llm.providers.<name>.model: default model name used for completions unless overridden by a module.",
  "llm_providers_organization": "llm.providers.<name>.organization: optional OpenAI organization ID for enterprise accounts.",
  "llm_providers_*_organization": "llm.providers.<name>.organization: optional OpenAI organization ID for enterprise accounts.",
  "llm_providers_api_version": "llm.providers.<name>.api_version: optional API version for Azure OpenAI (e.g., 2023-12-01-preview).",
  "llm_providers_*_api_version": "llm.providers.<name>.api_version: optional API version for Azure OpenAI.",
  "llm_providers_request_timeout": "llm.providers.<name>.request_timeout: request timeout in seconds for LLM calls.",
  "llm_providers_*_request_timeout": "llm.providers.<name>.request_timeout: request timeout in seconds for LLM calls.",
  "llm_providers_temperature": "llm.providers.<name>.temperature: sampling temperature between 0 and 2. Lower values keep answers focused; higher values add randomness. Details: https://platform.openai.com/docs/guides/text-generation/parameter-details.",
  "llm_providers_*_temperature": "llm.providers.<name>.temperature: sampling temperature between 0 and 2. Lower values keep answers focused; higher values add randomness.",
  "llm_providers_top_p": "llm.providers.<name>.top_p: nucleus sampling cutoff (probability mass) that limits which tokens are considered. Lower top_p narrows choices; higher values act like a wider search. Details: https://platform.openai.com/docs/guides/text-generation/parameter-details.",
  "llm_providers_*_top_p": "llm.providers.<name>.top_p: nucleus sampling cutoff (probability mass) that limits which tokens are considered.",
  "llm_providers_max_output_tokens": "llm.providers.<name>.max_output_tokens: maximum number of tokens the provider should return per response.",
  "llm_providers_*_max_output_tokens": "llm.providers.<name>.max_output_tokens: maximum number of tokens the provider should return per response.",
  "llm_providers_max_excerpt_lines": "llm.providers.<name>.max_excerpt_lines: limit on how many lines of context are sent with a request when modules omit their own override.",
  "llm_providers_*_max_excerpt_lines": "llm.providers.<name>.max_excerpt_lines: limit on how many lines of context are sent with a request when modules omit their own override.",
  "rag": "rag: Retrieval-Augmented Generation configuration for enhanced AI analysis using documentation context from git repositories.",
  "rag_enabled": "rag.enabled: master switch to enable RAG functionality for retrieving relevant documentation during LLM analysis.",
  "rag_cache_dir": "rag.cache_dir: directory for caching downloaded git repositories and processed data.",
  "rag_vector_store_dir": "rag.vector_store_dir: directory where the vector database stores embeddings and metadata.",
  "rag_embedding_model": "rag.embedding_model: name of the sentence-transformer model (e.g., 'sentence-transformers/all-MiniLM-L6-v2').",
  "rag_device": "rag.device: device to run embeddings on ('cpu' or 'cuda' for GPU acceleration).",
  "rag_batch_size": "rag.batch_size: number of texts to process at once (larger batches are faster but use more memory).",
  "rag_top_k": "rag.top_k: maximum number of document chunks to retrieve for each query.",
  "rag_similarity_threshold": "rag.similarity_threshold: minimum similarity score (0.0-1.0) for retrieved documents (higher = more relevant but fewer results).",
  "rag_max_chunks": "rag.max_chunks: maximum number of chunks to include in the final context sent to the LLM.",
  "pipelines": "pipelines: reusable bundles that marry grouping rules with a classifier. Modules pick a pipeline by name so you can share logic between different log sources.",
  "pipelines_name": "pipelines.name: unique identifier for the pipeline. A module references it via modules.pipeline.",
  "pipelines_match": "pipelines.match: file selection hints used when multiple pipelines exist and you want a given pipeline to claim only certain files.",
  "match_filename_regex": "match.filename_regex: regular expression that marks which log file paths belong to this pipeline.",
  "pipelines_grouping": "pipelines.grouping: describes how raw lines are split before classification. Useful when each entry spans multiple lines.",
  "grouping_type": "grouping.type: choose whole_file to treat the entire file as one chunk, marker to split on start and end regexes, or separator to split on run boundaries.",
  "grouping_start_regex": "grouping.start_regex: when grouping.type is marker, this regex identifies the first line of a chunk.",
  "grouping_end_regex": "grouping.end_regex: when grouping.type is marker, this regex identifies the last line of a chunk. Leave empty to let the file end close the final chunk.",
  "grouping_separator_regex": "grouping.separator_regex: when grouping.type is separator, this regex identifies run boundaries (e.g., start of rsnapshot execution). Use with only_last: true to process only the most recent run.",
  "separator_regex": "separator_regex: regex that identifies run boundaries when using separator-based grouping.",
  "pipelines_classifier": "pipelines.classifier: selects the classification engine and provides the regex lists that drive severity scoring.",
  "classifier_type": "classifier.type: implementation key such as regex_counter or rsnapshot_basic. Choose the classifier that matches your log format.",
  "classifier_error_regexes": "classifier.error_regexes: list of regex patterns that should count as errors when found in a chunk.",
  "classifier_warning_regexes": "classifier.warning_regexes: list of regex patterns that should count as warnings when found in a chunk.",
  "classifier_ignore_regexes": "classifier.ignore_regexes: list of regex patterns that should be completely skipped before scoring severity.",
  "modules": "modules: concrete jobs that watch a path using a chosen mode and pipeline. They decide how logs are read and where outputs are delivered.",
  "modules_name": "modules.name: friendly identifier for the module that appears in the CLI and Web UI drop downs.",
  "modules_enabled": "modules.enabled: on or off switch for the module. Disabled modules are skipped without error.",
  "modules_path": "modules.path: the source location to read. In batch mode it can be a single file or a directory, and directories are walked recursively. In follow mode this must be a single file.",
  "modules_mode": "modules.mode: batch reads everything once, follow tails a single growing log file with rotation awareness.",
  "modules_pipeline": "modules.pipeline: name of the pipeline to use for this module. References a pipeline defined in the pipelines section.",
  "modules_output_format": "modules.output_format: text prints human friendly findings to stdout, json emits a machine readable object per finding.",
  "modules_min_print_severity": "modules.min_print_severity: filter so that only findings at or above this severity are printed by the module regardless of what the classifier produced.",
  "modules_stale_after_minutes": "modules.stale_after_minutes: minutes after the module's log activity is considered stale in the dashboard and AI logs explorer; defaults to LOGTRIAGE_INGESTION_STALENESS_MINUTES when omitted.",
  "modules_llm": "modules.llm: LLM settings for this module including provider selection, prompt template path, payload emission directory, and excerpt limits.",
  "modules_llm_enabled": "modules.llm.enabled: enable/disable LLM processing for this specific module.",
  "modules_llm_provider": "modules.llm.provider: optional provider name. If omitted, llm.default_provider is used or the lone provider is chosen automatically.",
  "modules_llm_min_severity": "modules.llm.min_severity: minimum severity level for automatic LLM processing of findings.",
  "modules_llm_prompt_template": "modules.llm.prompt_template: filesystem path to the prompt template used when constructing the LLM request payload.",
  "modules_llm_emit_llm_payloads_dir": "modules.llm.emit_llm_payloads_dir: directory where generated LLM payload files and responses should be saved for this module.",
  "modules_llm_max_excerpt_lines": "modules.llm.max_excerpt_lines: per-module override for how many lines are kept in the excerpt sent to the LLM.",
  "modules_llm_context_prefix_lines": "modules.llm.context_prefix_lines: number of lines to prepend before a matched finding line when building the excerpt for the LLM.",
  "modules_llm_context_suffix_lines": "modules.llm.context_suffix_lines: number of lines to append after a matched finding line when building the excerpt for the LLM.",
  "modules_llm_max_output_tokens": "modules.llm.max_output_tokens: per-module override for the token cap on completions.",
  "modules_stream": "modules.stream: stream and tailing controls that apply when modules.mode is follow.",
  "modules_stream_from_beginning": "modules.stream.from_beginning: when true, reads the whole file before tailing. Set to false for a tail-only experience similar to tail -F.",
  "modules_stream_interval": "modules.stream.interval: poll frequency in seconds for new lines and rotation detection in follow mode.",
  "modules_alerts": "modules.alerts: webhook and MQTT delivery points that can be tailored per module with their own severity thresholds.",
  "modules_alerts_webhook": "modules.alerts.webhook: HTTP callback settings used when a finding should be pushed to an external service.",
  "modules_alerts_webhook_enabled": "modules.alerts.webhook.enabled: enable/disable webhook alerts for this module.",
  "modules_alerts_webhook_url": "modules.alerts.webhook.url: endpoint URL for webhook notifications.",
  "modules_alerts_webhook_method": "modules.alerts.webhook.method: HTTP method for webhook requests (GET, POST, PUT, DELETE).",
  "modules_alerts_webhook_min_severity": "modules.alerts.webhook.min_severity: minimum severity level to trigger webhook alerts.",
  "modules_alerts_webhook_headers": "modules.alerts.webhook.headers: custom HTTP headers to include with webhook requests.",
  "modules_alerts_mqtt": "modules.alerts.mqtt: configuration for publishing findings to an MQTT broker.",
  "modules_alerts_mqtt_enabled": "modules.alerts.mqtt.enabled: enable/disable MQTT alerts for this module.",
  "modules_alerts_mqtt_host": "modules.alerts.mqtt.host: MQTT broker hostname or IP address.",
  "modules_alerts_mqtt_port": "modules.alerts.mqtt.port: MQTT broker port (default: 1883).",
  "modules_alerts_mqtt_topic": "modules.alerts.mqtt.topic: MQTT topic to publish alert messages to.",
  "modules_alerts_mqtt_username": "modules.alerts.mqtt.username: optional username for MQTT authentication.",
  "modules_alerts_mqtt_password": "modules.alerts.mqtt.password: optional password for MQTT authentication.",
  "modules_alerts_mqtt_min_severity": "modules.alerts.mqtt.min_severity: minimum severity level to trigger MQTT alerts.",
  "modules_rag": "modules.rag: module-specific RAG configuration for documentation retrieval.",
  "modules_rag_enabled": "modules.rag.enabled: enable/disable RAG for this specific module.",
  "modules_rag_knowledge_sources": "modules.rag.knowledge_sources: list of git repositories containing documentation for this module.",
  "modules_rag_knowledge_sources_repo_url": "modules.rag.knowledge_sources.repo_url: git repository URL containing documentation.",
  "modules_rag_knowledge_sources_branch": "modules.rag.knowledge_sources.branch: git branch to use (default: main).",
  "modules_rag_knowledge_sources_include_paths": "modules.rag.knowledge_sources.include_paths: list of glob patterns for selecting files from the repository.",
  "database": "database: connection string and retention policy used by both CLI and Web UI to store findings history.",
  "database_url": "database.url: SQLAlchemy connection URL such as sqlite:///file.db or a PostgreSQL DSN.",
  "database_retention_days": "database.retention_days: number of days to keep stored findings before cleanup routines prune them.",
  "webui": "webui: HTTP server options including host, port, base_path, and security controls like secret_key, allowed_ips, and admin_users.",
  "webui_enabled": "webui.enabled: master switch to turn the Web UI on or off.",
  "webui_host": "webui.host: IP address to bind the web server to (0.0.0.0 for all interfaces).",
  "webui_port": "webui.port: port number for the web server to listen on.",
  "webui_base_path": "webui.base_path: URL base path (useful for reverse proxies).",
  "webui_secret_key": "webui.secret_key: secret key used for session encryption (change from default for security).",
  "webui_session_cookie_name": "webui.session_cookie_name: name of the session cookie.",
  "webui_dark_mode_default": "webui.dark_mode_default: whether to default to dark theme in the UI.",
  "webui_allowed_ips": "webui.allowed_ips: list of IP addresses allowed to access the UI (empty = allow all).",
  "webui_admin_users": "webui.admin_users: list of admin users with username and password hash for authentication.",
  "logging": "logging: configure application logging levels and outputs for debugging and monitoring.",
  "logging_level": "logging.level: global logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).",
  "logging_format": "logging.format: log message format string.",
  "logging_file": "logging.file: path to log file (comment out to only log to stdout/stderr).",
  "logging_loggers": "logging.loggers: configure specific loggers with different levels for fine-grained control."
}

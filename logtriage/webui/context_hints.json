{
  "root": "Configuration is grouped the same way as the README: llm, pipelines, modules, stream, database, webui, and alerts.",
  "llm": "llm: global LLM defaults and provider definitions shared by every module.",
  "llm_enabled": "llm.enabled: master switch that allows modules to call an OpenAI-compatible provider when a finding needs LLM analysis.",
  "llm_min_severity": "llm.min_severity: lowest severity that should be eligible for LLM processing unless a module overrides it.",
  "llm_default_provider": "llm.default_provider: name of the provider to use when modules do not specify one; if omitted and only one provider exists it is picked automatically.",
  "llm_providers": "llm.providers.<name>: OpenAI/vLLM compatible endpoint definition with api_base, api_key_env, model, and optional organization/api_version/request_timeout/temperature/top_p/max_output_tokens/max_excerpt_lines. See OpenAI's parameter guide for how sampling works: https://platform.openai.com/docs/guides/text-generation/parameter-details.",
  "llm_providers_*": "llm.providers.<name>: OpenAI/vLLM compatible endpoint definition. Use api_base, api_key_env, model, and optional parameters.",
  "llm_providers_api_base": "llm.providers.<name>.api_base: base URL of the OpenAI-compatible endpoint (e.g., https://api.openai.com/v1).",
  "llm_providers_*_api_base": "llm.providers.<name>.api_base: base URL of the OpenAI-compatible endpoint (e.g., https://api.openai.com/v1).",
  "llm_providers_api_key_env": "llm.providers.<name>.api_key_env: environment variable that holds the API key used for this provider.",
  "llm_providers_*_api_key_env": "llm.providers.<name>.api_key_env: environment variable that holds the API key used for this provider.",
  "llm_providers_model": "llm.providers.<name>.model: default model name used for completions unless overridden by a module.",
  "llm_providers_*_model": "llm.providers.<name>.model: default model name used for completions unless overridden by a module.",
  "llm_providers_request_timeout": "llm.providers.<name>.request_timeout: request timeout in seconds for LLM calls.",
  "llm_providers_*_request_timeout": "llm.providers.<name>.request_timeout: request timeout in seconds for LLM calls.",
  "llm_providers_temperature": "llm.providers.<name>.temperature: sampling temperature between 0 and 2. Lower values keep answers focused; higher values add randomness. Details: https://platform.openai.com/docs/guides/text-generation/parameter-details.",
  "llm_providers_*_temperature": "llm.providers.<name>.temperature: sampling temperature between 0 and 2. Lower values keep answers focused; higher values add randomness.",
  "llm_providers_top_p": "llm.providers.<name>.top_p: nucleus sampling cutoff (probability mass) that limits which tokens are considered. Lower top_p narrows choices; higher values act like a wider search. Details: https://platform.openai.com/docs/guides/text-generation/parameter-details.",
  "llm_providers_*_top_p": "llm.providers.<name>.top_p: nucleus sampling cutoff (probability mass) that limits which tokens are considered.",
  "llm_providers_max_output_tokens": "llm.providers.<name>.max_output_tokens: maximum number of tokens the provider should return per response.",
  "llm_providers_*_max_output_tokens": "llm.providers.<name>.max_output_tokens: maximum number of tokens the provider should return per response.",
  "llm_providers_max_excerpt_lines": "llm.providers.<name>.max_excerpt_lines: limit on how many lines of context are sent with a request when modules omit their own override.",
  "llm_providers_*_max_excerpt_lines": "llm.providers.<name>.max_excerpt_lines: limit on how many lines of context are sent with a request when modules omit their own override.",
  "api_base": "api_base: base URL of the OpenAI-compatible endpoint (e.g., https://api.openai.com/v1).",
  "api_key_env": "api_key_env: environment variable that holds the API key used for this provider.",
  "model": "model: default model name used for completions.",
  "request_timeout": "request_timeout: request timeout in seconds for LLM calls.",
  "temperature": "temperature: sampling temperature between 0 and 2. Lower values keep answers focused; higher values add randomness.",
  "top_p": "top_p: nucleus sampling cutoff (probability mass) that limits which tokens are considered.",
  "max_output_tokens": "max_output_tokens: maximum number of tokens the provider should return per response.",
  "max_excerpt_lines": "max_excerpt_lines: limit on how many lines of context are sent with a request.",
  "pipelines": "pipelines: reusable bundles that marry grouping rules with a classifier. Modules pick a pipeline by name so you can share logic between different log sources.",
  "pipelines_name": "pipelines.name: unique identifier for the pipeline. A module references it via modules.pipeline.",
  "pipelines_match": "pipelines.match: file selection hints used when multiple pipelines exist and you want a given pipeline to claim only certain files.",
  "match_filename_regex": "match.filename_regex: regular expression that marks which log file paths belong to this pipeline.",
  "pipelines_grouping": "pipelines.grouping: describes how raw lines are split before classification. Useful when each entry spans multiple lines.",
  "grouping_type": "grouping.type: choose whole_file to treat the entire file as one chunk, marker to split on start and end regexes, or separator to split on run boundaries.",
  "grouping_start_regex": "grouping.start_regex: when grouping.type is marker, this regex identifies the first line of a chunk.",
  "grouping_end_regex": "grouping.end_regex: when grouping.type is marker, this regex identifies the last line of a chunk. Leave empty to let the file end close the final chunk.",
  "grouping_separator_regex": "grouping.separator_regex: when grouping.type is separator, this regex identifies run boundaries (e.g., start of rsnapshot execution). Use with only_last: true to process only the most recent run.",
  "separator_regex": "separator_regex: regex that identifies run boundaries when using separator-based grouping.",
  "pipelines_classifier": "pipelines.classifier: selects the classification engine and provides the regex lists that drive severity scoring.",
  "classifier_type": "classifier.type: implementation key such as regex_counter or rsnapshot_basic. Choose the classifier that matches your log format.",
  "classifier_error_regexes": "classifier.error_regexes: list of regex patterns that should count as errors when found in a chunk.",
  "classifier_warning_regexes": "classifier.warning_regexes: list of regex patterns that should count as warnings when found in a chunk.",
  "classifier_ignore_regexes": "classifier.ignore_regexes: list of regex patterns that should be completely skipped before scoring severity.",
  "modules": "modules: concrete jobs that watch a path using a chosen mode and pipeline. They decide how logs are read and where outputs are delivered.",
  "modules_llm": "modules.llm: LLM settings for this module including provider selection, prompt template path, payload emission directory, and excerpt limits.",
  "modules_llm_provider": "modules.llm.provider: optional provider name. If omitted, llm.default_provider is used or the lone provider is chosen automatically.",
  "modules_llm_prompt_template": "modules.llm.prompt_template: filesystem path to the prompt template used when constructing the LLM request payload.",
  "modules_llm_emit_payloads_dir": "modules.llm.emit_llm_payloads_dir: directory where generated LLM payload files and responses should be saved for this module.",
  "modules_llm_max_excerpt_lines": "modules.llm.max_excerpt_lines: per-module override for how many lines are kept in the excerpt sent to the LLM.",
  "modules_llm_context_prefix_lines": "modules.llm.context_prefix_lines: number of lines to prepend before a matched finding line when building the excerpt for the LLM.",
  "modules_llm_context_suffix_lines": "modules.llm.context_suffix_lines: number of lines to append after a matched finding line when building the excerpt for the LLM.",
  "llm_context_prefix_lines": "llm.context_prefix_lines: global default for number of lines to prepend before a matched finding line when building the excerpt for the LLM.",
  "llm_context_suffix_lines": "llm.context_suffix_lines: global default for number of lines to append after a matched finding line when building the excerpt for the LLM.",
  "context_prefix_lines": "context_prefix_lines: number of lines to prepend before a matched finding line when building the excerpt for the LLM.",
  "context_suffix_lines": "context_suffix_lines: number of lines to append after a matched finding line when building the excerpt for the LLM.",
  "modules_llm_max_output_tokens": "modules.llm.max_output_tokens: per-module override for the token cap on completions.",
  "modules_emit_llm_payloads_dir": "modules.llm.emit_llm_payloads_dir: directory where generated LLM payload files should be saved for this module.",
  "modules_name": "modules.name: friendly identifier for the module that appears in the CLI and Web UI drop downs.",
  "modules_enabled": "modules.enabled: on or off switch for the module. Disabled modules are skipped without error.",
  "modules_path": "modules.path: the source location to read. In batch mode it can be a single file or a directory, and directories are walked recursively. In follow mode this must be a single file.",
  "modules_mode": "modules.mode: batch reads everything once, follow tails a single growing log file with rotation awareness.",
  "modules_output_format": "modules.output_format: text prints human friendly findings to stdout, json emits a machine readable object per finding.",
  "modules_min_print_severity": "modules.min_print_severity: filter so that only findings at or above this severity are printed by the module regardless of what the classifier produced.",
  "modules_stale_after_minutes": "modules.stale_after_minutes: minutes after the module's log activity is considered stale in the dashboard and AI logs explorer; defaults to LOGTRIAGE_INGESTION_STALENESS_MINUTES when omitted.",
  "modules_alerts": "modules.alerts: webhook and MQTT delivery points that can be tailored per module with their own severity thresholds.",
  "modules_pipeline": "modules.pipeline: name of the pipeline to use for this module. References a pipeline defined in the pipelines section.",
  "pipeline": "pipeline: name of the pipeline to use for this module.",
  "name": "name: friendly identifier for this item.",
  "path": "path: filesystem path to the log file or directory.",
  "mode": "mode: batch reads everything once, follow tails a single growing log file.",
  "enabled": "enabled: on or off switch. Disabled items are skipped without error.",
  "type": "type: implementation type selector.",
  "error_regexes": "error_regexes: list of regex patterns that should count as errors.",
  "warning_regexes": "warning_regexes: list of regex patterns that should count as warnings.",
  "ignore_regexes": "ignore_regexes: list of regex patterns that should be completely skipped.",
  "provider": "provider: optional provider name. If omitted, the default provider is used.",
  "prompt_template": "prompt_template: filesystem path to the prompt template.",
  "emit_llm_payloads_dir": "emit_llm_payloads_dir: directory where generated LLM payload files should be saved.",
  "min_severity": "min_severity: minimum severity level for this operation (WARNING, ERROR, or CRITICAL).",
  "output_format": "output_format: text prints human friendly output, json emits machine readable objects.",
  "stale_after_minutes": "stale_after_minutes: minutes after which activity is considered stale.",
  "alerts_webhook": "alerts.webhook: HTTP callback settings used when a finding should be pushed to an external service.",
  "alerts_mqtt": "alerts.mqtt: configuration for publishing findings to an MQTT broker.",
  "modules_stream": "modules.stream: stream and tailing controls that apply when modules.mode is follow.",
  "database": "database: connection string and retention policy used by both CLI and Web UI to store findings history.",
  "database_url": "database.url: SQLAlchemy connection URL such as sqlite:///file.db or a PostgreSQL DSN.",
  "database_retention_days": "database.retention_days: number of days to keep stored findings before cleanup routines prune them.",
  "webui": "webui: HTTP server options including host, port, base_path, and security controls like secret_key, allowed_ips, and admin_users.",
  "webui_enabled": "webui.enabled: master switch to turn the Web UI on or off."
}

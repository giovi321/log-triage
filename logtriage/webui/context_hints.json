{
  "root": "Configuration is grouped the same way as the README: llm, pipelines, modules, baseline, stream, database, webui, and alerts.",
  "llm": "llm: global LLM defaults and provider definitions shared by every module.",
  "llm_enabled": "llm.enabled: master switch that allows modules to call an OpenAI-compatible provider when a finding needs LLM analysis.",
  "llm_min_severity": "llm.min_severity: lowest severity that should be eligible for LLM processing unless a module overrides it.",
  "llm_default_provider": "llm.default_provider: name of the provider to use when modules do not specify one; if omitted and only one provider exists it is picked automatically.",
  "llm_providers": "llm.providers.<name>: OpenAI/vLLM compatible endpoint definition with api_base, api_key_env, model, and optional organization/api_version/request_timeout/temperature/top_p/max_output_tokens/max_excerpt_lines.",
  "pipelines": "pipelines: reusable bundles that marry grouping rules with a classifier. Modules pick a pipeline by name so you can share logic between different log sources.",
  "pipelines_name": "pipelines.name: unique identifier for the pipeline. A module references it via modules.pipeline.",
  "pipelines_match": "pipelines.match: file selection hints used when multiple pipelines exist and you want a given pipeline to claim only certain files.",
  "match_filename_regex": "match.filename_regex: regular expression that marks which log file paths belong to this pipeline.",
  "pipelines_grouping": "pipelines.grouping: describes how raw lines are split before classification. Useful when each entry spans multiple lines.",
  "grouping_type": "grouping.type: choose whole_file to treat the entire file as one chunk or marker to split on start and end regexes.",
  "grouping_start_regex": "grouping.start_regex: when grouping.type is marker, this regex identifies the first line of a chunk.",
  "grouping_end_regex": "grouping.end_regex: when grouping.type is marker, this regex identifies the last line of a chunk. Leave empty to let the file end close the final chunk.",
  "pipelines_classifier": "pipelines.classifier: selects the classification engine and provides the regex lists that drive severity scoring.",
  "classifier_type": "classifier.type: implementation key such as regex_counter or rsnapshot_basic. Choose the classifier that matches your log format.",
  "classifier_error_regexes": "classifier.error_regexes: list of regex patterns that should count as errors when found in a chunk.",
  "classifier_warning_regexes": "classifier.warning_regexes: list of regex patterns that should count as warnings when found in a chunk.",
  "classifier_ignore_regexes": "classifier.ignore_regexes: list of regex patterns that should be completely skipped before scoring severity.",
  "modules": "modules: concrete jobs that watch a path using a chosen mode and pipeline. They decide how logs are read and where outputs are delivered.",
  "modules_llm": "modules.llm: LLM settings for this module including provider selection, prompt template path, payload emission directory, and excerpt limits.",
  "modules_llm_provider": "modules.llm.provider: optional provider name. If omitted, llm.default_provider is used or the lone provider is chosen automatically.",
  "modules_llm_prompt_template": "modules.llm.prompt_template: filesystem path to the prompt template used when constructing the LLM request payload.",
  "modules_llm_emit_payloads_dir": "modules.llm.emit_llm_payloads_dir: directory where generated LLM payload files and responses should be saved for this module.",
  "modules_llm_max_excerpt_lines": "modules.llm.max_excerpt_lines: per-module override for how many lines are kept in the excerpt sent to the LLM.",
  "modules_llm_context_prefix_lines": "modules.llm.context_prefix_lines: number of lines to prepend before a matched finding line when building the excerpt for the LLM.",
  "modules_llm_max_output_tokens": "modules.llm.max_output_tokens: per-module override for the token cap on completions.",
  "modules_emit_llm_payloads_dir": "modules.llm.emit_llm_payloads_dir: directory where generated LLM payload files should be saved for this module.",
  "modules_name": "modules.name: friendly identifier for the module that appears in the CLI and Web UI drop downs.",
  "modules_enabled": "modules.enabled: on or off switch for the module. Disabled modules are skipped without error.",
  "modules_path": "modules.path: the source location to read. In batch mode it can be a single file or a directory, and directories are walked recursively. In follow mode this must be a single file.",
  "modules_mode": "modules.mode: batch reads everything once, follow tails a single growing log file with rotation awareness.",
  "modules_output_format": "modules.output_format: text prints human friendly findings to stdout, json emits a machine readable object per finding.",
  "modules_min_print_severity": "modules.min_print_severity: filter so that only findings at or above this severity are printed by the module regardless of what the classifier produced.",
  "modules_exit_code_by_severity": "modules.exit_code_by_severity: map of severity to exit code for automation.",
  "modules_alerts": "modules.alerts: webhook and MQTT delivery points that can be tailored per module with their own severity thresholds.",
  "alerts_webhook": "alerts.webhook: HTTP callback settings used when a finding should be pushed to an external service.",
  "alerts_mqtt": "alerts.mqtt: configuration for publishing findings to an MQTT broker.",
  "modules_baseline": "modules.baseline: anomaly detection settings that compare current counts to historical averages and bump severity when counts spike.",
  "modules_stream": "modules.stream: stream and tailing controls that apply when modules.mode is follow.",
  "database": "database: connection string and retention policy used by both CLI and Web UI to store findings history.",
  "database_url": "database.url: SQLAlchemy connection URL such as sqlite:///file.db or a PostgreSQL DSN.",
  "database_retention_days": "database.retention_days: number of days to keep stored findings before cleanup routines prune them.",
  "webui": "webui: HTTP server options including host, port, base_path, and security controls like secret_key, allowed_ips, and admin_users.",
  "webui_enabled": "webui.enabled: master switch to turn the Web UI on or off."
}

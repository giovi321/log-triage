<!DOCTYPE html>
<html>
<head>
    <title>Debug Context Hints</title>
</head>
<body>
    <h1>Context Hints Debug</h1>
    <div id="output"></div>
    
    <script>
        // Simulate the context hints data
        const hintTexts = {"root":"Configuration is grouped the same way as the README: llm, pipelines, modules, stream, database, webui, and alerts.","llm":"llm: global LLM defaults and provider definitions shared by every module.","llm_enabled":"llm.enabled: master switch that allows modules to call an OpenAI-compatible provider when a finding needs LLM analysis.","llm_min_severity":"llm.min_severity: lowest severity that should be eligible for LLM processing unless a module overrides it.","llm_default_provider":"llm.default_provider: name of the provider to use when modules do not specify one; if omitted and only one provider exists it is picked automatically.","llm_providers":"llm.providers.<name>: OpenAI/vLLM compatible endpoint definition with api_base, api_key_env, model, and optional organization/api_version/request_timeout/temperature/top_p/max_output_tokens/max_excerpt_lines. See OpenAI's parameter guide for how sampling works: https://platform.openai.com/docs/guides/text-generation/parameter-details.","llm_providers_*":"llm.providers.<name>: OpenAI/vLLM compatible endpoint definition. Use api_base, api_key_env, model, and optional parameters.","llm_providers_api_base":"llm.providers.<name>.api_base: base URL of the OpenAI-compatible endpoint (e.g., https://api.openai.com/v1).","llm_providers_*_api_base":"llm.providers.<name>.api_base: base URL of the OpenAI-compatible endpoint (e.g., https://api.openai.com/v1).","llm_providers_api_key_env":"llm.providers.<name>.api_key_env: environment variable that holds the API key used for this provider.","llm_providers_*_api_key_env":"llm.providers.<name>.api_key_env: environment variable that holds the API key used for this provider.","llm_providers_model":"llm.providers.<name>.model: default model name used for completions unless overridden by a module.","llm_providers_*_model":"llm.providers.<name>.model: default model name used for completions unless overridden by a module.","llm_providers_request_timeout":"llm.providers.<name>.request_timeout: request timeout in seconds for LLM calls.","llm_providers_*_request_timeout":"llm.providers.<name>.request_timeout: request timeout in seconds for LLM calls.","llm_providers_temperature":"llm.providers.<name>.temperature: sampling temperature between 0 and 2. Lower values keep answers focused; higher values add randomness. Details: https://platform.openai.com/docs/guides/text-generation/parameter-details.","llm_providers_*_temperature":"llm.providers.<name>.temperature: sampling temperature between 0 and 2. Lower values keep answers focused; higher values add randomness.","llm_providers_top_p":"llm.providers.<name>.top_p: nucleus sampling cutoff (probability mass) that limits which tokens are considered. Lower top_p narrows choices; higher values act like a wider search. Details: https://platform.openai.com/docs/guides/text-generation/parameter-details.","llm_providers_*_top_p":"llm.providers.<name>.top_p: nucleus sampling cutoff (probability mass) that limits which tokens are considered.","llm_providers_max_output_tokens":"llm.providers.<name>.max_output_tokens: maximum number of tokens the provider should return per response.","llm_providers_*_max_output_tokens":"llm.providers.<name>.max_output_tokens: maximum number of tokens the provider should return per response.","llm_providers_max_excerpt_lines":"llm.providers.<name>.max_excerpt_lines: limit on how many lines of context are sent with a request when modules omit their own override.","llm_providers_*_max_excerpt_lines":"llm.providers.<name>.max_excerpt_lines: limit on how many lines of context are sent with a request when modules omit their own override.","api_base":"api_base: base URL of the OpenAI-compatible endpoint (e.g., https://api.openai.com/v1).","api_key_env":"api_key_env: environment variable that holds the API key used for this provider.","model":"model: default model name used for completions.","request_timeout":"request_timeout: request timeout in seconds for LLM calls.","temperature":"temperature: sampling temperature between 0 and 2. Lower values keep answers focused; higher values add randomness.","top_p":"top_p: nucleus sampling cutoff (probability mass) that limits which tokens are considered.","max_output_tokens":"max_output_tokens: maximum number of tokens the provider should return per response.","max_excerpt_lines":"max_excerpt_lines: limit on how many lines of context are sent with a request.","pipelines":"pipelines: reusable bundles that marry grouping rules with a classifier. Modules pick a pipeline by name so you can share logic between different log sources.","pipelines_name":"pipelines.name: unique identifier for the pipeline. A module references it via modules.pipeline.","pipelines_match":"pipelines.match: file selection hints used when multiple pipelines exist and you want a given pipeline to claim only certain files.","match_filename_regex":"match.filename_regex: regular expression that marks which log file paths belong to this pipeline.","pipelines_grouping":"pipelines.grouping: describes how raw lines are split before classification. Useful when each entry spans multiple lines.","grouping_type":"grouping.type: choose whole_file to treat the entire file as one chunk, marker to split on start and end regexes, or separator to split on run boundaries.","grouping_start_regex":"grouping.start_regex: when grouping.type is marker, this regex identifies the first line of a chunk.","grouping_end_regex":"grouping.end_regex: when grouping.type is marker, this regex identifies the last line of a chunk. Leave empty to let the file end close the final chunk.","grouping_separator_regex":"grouping.separator_regex: when grouping.type is separator, this regex identifies run boundaries (e.g., start of rsnapshot execution). Use with only_last: true to process only the most recent run.","separator_regex":"separator_regex: regex that identifies run boundaries when using separator-based grouping.","pipelines_classifier":"pipelines.classifier: selects the classification engine and provides the regex lists that drive severity scoring.","classifier_type":"classifier.type: implementation key such as regex_counter or rsnapshot_basic. Choose the classifier that matches your log format.","classifier_error_regexes":"classifier.error_regexes: list of regex patterns that should count as errors when found in a chunk.","classifier_warning_regexes":"classifier.warning_regexes: list of regex patterns that should count as warnings when found in a chunk.","classifier_ignore_regexes":"classifier.ignore_regexes: list of regex patterns that should be completely skipped before scoring severity.","modules":"modules: concrete jobs that watch a path using a chosen mode and pipeline. They decide how logs are read and where outputs are delivered.","modules_llm":"modules.llm: LLM settings for this module including provider selection, prompt template path, payload emission directory, and excerpt limits.","modules_llm_provider":"modules.llm.provider: optional provider name. If omitted, llm.default_provider is used or the lone provider is chosen automatically.","modules_llm_prompt_template":"modules.llm.prompt_template: filesystem path to the prompt template used when constructing the LLM request payload.","modules_llm_emit_payloads_dir":"modules.llm.emit_llm_payloads_dir: directory where generated LLM payload files and responses should be saved for this module.","modules_llm_max_excerpt_lines":"modules.llm.max_excerpt_lines: per-module override for how many lines are kept in the excerpt sent to the LLM.","modules_llm_context_prefix_lines":"modules.llm.context_prefix_lines: number of lines to prepend before a matched finding line when building the excerpt for the LLM.","modules_llm_context_suffix_lines":"modules.llm.context_suffix_lines: number of lines to append after a matched finding line when building the excerpt for the LLM.","llm_context_prefix_lines":"llm.context_prefix_lines: global default for number of lines to prepend before a matched finding line when building the excerpt for the LLM.","llm_context_suffix_lines":"llm.context_suffix_lines: global default for number of lines to append after a matched finding line when building the excerpt for the LLM.","context_prefix_lines":"context_prefix_lines: number of lines to prepend before a matched finding line when building the excerpt for the LLM.","context_suffix_lines":"context_suffix_lines: number of lines to append after a matched finding line when building the excerpt for the LLM.","modules_llm_max_output_tokens":"modules.llm.max_output_tokens: per-module override for the token cap on completions.","modules_emit_llm_payloads_dir":"modules.llm.emit_llm_payloads_dir: directory where generated LLM payload files should be saved for this module.","modules_name":"modules.name: friendly identifier for the module that appears in the CLI and Web UI drop downs.","modules_enabled":"modules.enabled: on or off switch for the module. Disabled modules are skipped without error.","modules_path":"modules.path: the source location to read. In batch mode it can be a single file or a directory, and directories are walked recursively. In follow mode this must be a single file.","modules_mode":"modules.mode: batch reads everything once, follow tails a single growing log file with rotation awareness.","modules_output_format":"modules.output_format: text prints human friendly findings to stdout, json emits a machine readable object per finding.","modules_min_print_severity":"modules.min_print_severity: filter so that only findings at or above this severity are printed by the module regardless of what the classifier produced.","modules_stale_after_minutes":"modules.stale_after_minutes: minutes after the module's log activity is considered stale in the dashboard and AI logs explorer; defaults to LOGTRIAGE_INGESTION_STALENESS_MINUTES when omitted.","modules_alerts":"modules.alerts: webhook and MQTT delivery points that can be tailored per module with their own severity thresholds.","modules_pipeline":"modules.pipeline: name of the pipeline to use for this module. References a pipeline defined in the pipelines section.","pipeline":"pipeline: name of the pipeline to use for this module.","name":"name: friendly identifier for this item.","path":"path: filesystem path to the log file or directory.","mode":"mode: batch reads everything once, follow tails a single growing log file.","enabled":"enabled: on or off switch. Disabled items are skipped without error.","type":"type: implementation type selector.","error_regexes":"error_regexes: list of regex patterns that should count as errors.","warning_regexes":"warning_regexes: list of regex patterns that should count as warnings.","ignore_regexes":"ignore_regexes: list of regex patterns that should be completely skipped.","provider":"provider: optional provider name. If omitted, the default provider is used.","prompt_template":"prompt_template: filesystem path to the prompt template.","emit_llm_payloads_dir":"emit_llm_payloads_dir: directory where generated LLM payload files should be saved.","min_severity":"min_severity: minimum severity level for this operation (WARNING, ERROR, or CRITICAL).","output_format":"output_format: text prints human friendly output, json emits machine readable objects.","stale_after_minutes":"stale_after_minutes: minutes after which activity is considered stale.","alerts_webhook":"alerts.webhook: HTTP callback settings used when a finding should be pushed to an external service.","alerts_mqtt":"alerts.mqtt: configuration for publishing findings to an MQTT broker.","modules_stream":"modules.stream: stream and tailing controls that apply when modules.mode is follow.","database":"database: connection string and retention policy used by both CLI and Web UI to store findings history.","database_url":"database.url: SQLAlchemy connection URL such as sqlite:///file.db or a PostgreSQL DSN.","database_retention_days":"database.retention_days: number of days to keep stored findings before cleanup routines prune them.","webui":"webui: HTTP server options including host, port, base_path, and security controls like secret_key, allowed_ips, and admin_users.","webui_enabled":"webui.enabled: master switch to turn the Web UI on or off."};
        
        const safeHints = hintTexts && typeof hintTexts === 'object' ? hintTexts : {};
        const hintsLoaded = Object.keys(safeHints).length > 0;
        const defaultHint = safeHints.root || 'Top-level settings for logtriage.';
        
        console.log('[Debug] Hints loaded:', hintsLoaded, 'keys:', Object.keys(safeHints).length);
        
        function normalizeSegment(raw) {
            return raw.replace(/[^A-Za-z0-9_-]/g, '');
        }

        function firstWordForLine(line) {
            const trimmed = line.trim();
            if (!trimmed || trimmed.startsWith('#')) return null;
            const withoutDash = trimmed.replace(/^-\s*/, '');
            const match = withoutDash.match(/^([A-Za-z0-9_-]+)/);
            return match ? normalizeSegment(match[1]) : null;
        }

        function pathForCursor(doc, lineNumber) {
            const lines = doc.split('\n');
            const stack = [];
            // Only iterate up to (but not including) the current line to get ancestors
            for (let i = 0; i < lineNumber && i < lines.length; i++) {
                const raw = lines[i];
                const indentMatch = raw.match(/^(\s*)/);
                const indent = indentMatch ? indentMatch[1].length : 0;
                const trimmed = raw.trim();
                if (!trimmed || trimmed.startsWith('#')) continue;

                const isListItem = /^-\s*/.test(trimmed);
                const keyPart = isListItem ? trimmed.replace(/^-\s*/, '') : trimmed;
                const effectiveIndent = indent + (isListItem ? 1 : 0);

                const keyMatch = keyPart.match(/^([A-Za-z0-9_-]+):/);
                if (!keyMatch) continue;
                const key = normalizeSegment(keyMatch[1]);

                while (stack.length && stack[stack.length - 1].indent >= effectiveIndent) {
                    stack.pop();
                }
                stack.push({ key, indent: effectiveIndent });
            }
            
            // Now check the current line's indent to pop ancestors that are at same or deeper level
            if (lineNumber < lines.length) {
                const currentLine = lines[lineNumber];
                const currentIndentMatch = currentLine.match(/^(\s*)/);
                const currentIndent = currentIndentMatch ? currentIndentMatch[1].length : 0;
                const currentTrimmed = currentLine.trim();
                const currentIsListItem = /^-\s*/.test(currentTrimmed);
                const currentEffectiveIndent = currentIndent + (currentIsListItem ? 1 : 0);
                
                while (stack.length && stack[stack.length - 1].indent >= currentEffectiveIndent) {
                    stack.pop();
                }
            }
            
            return stack.map(s => s.key);
        }

        function hintForLine(line, doc, lineNumber) {
            const ancestors = pathForCursor(doc, lineNumber);
            const firstWord = firstWordForLine(line);

            const candidates = [];
            if (firstWord) {
                const normalizedAncestors = ancestors.map(normalizeSegment).filter(Boolean);

                // Most specific: full path with ancestors and current key
                for (let i = normalizedAncestors.length; i >= 0; i--) {
                    const joined = [...normalizedAncestors.slice(0, i), normalizeSegment(firstWord)]
                        .filter(Boolean)
                        .join('_');
                    if (joined) {
                        candidates.push(joined);
                    }
                }
            }

            // Backwards compatibility with older hint keys like "modules_name"
            if (firstWord) {
                for (let i = ancestors.length - 1; i >= 0; i--) {
                    const prefix = normalizeSegment(ancestors[i]);
                    if (prefix) {
                        candidates.push(`${prefix}_${firstWord}`);
                    }
                }
                candidates.push(firstWord);
            }

            // Exact matches first
            for (const key of candidates) {
                if (key && safeHints[key]) {
                    return safeHints[key];
                }
            }

            // Wildcard matches (e.g., llm_providers_*_api_base)
            const wildcardKeys = Object.keys(safeHints).filter((key) => key.includes('*'));
            for (const candidate of candidates) {
                for (const wildcard of wildcardKeys) {
                    const regex = new RegExp('^' + wildcard.replace(/\*/g, '[^_]+') + '$');
                    if (regex.test(candidate)) {
                        return safeHints[wildcard];
                    }
                }
            }

            return safeHints.root || defaultHint;
        }

        // Test with sample YAML content
        const sampleYaml = `llm:
  enabled: true
  min_severity: WARNING
  default_provider: openai
  providers:
    openai:
      api_base: https://api.openai.com/v1
      api_key_env: OPENAI_API_KEY
      model: gpt-4

modules:
  mymodule:
    name: test module
    enabled: true
    path: /var/log/app.log
    mode: follow
`;

        const output = document.getElementById('output');
        
        // Test each line
        const lines = sampleYaml.split('\n');
        lines.forEach((line, index) => {
            const hint = hintForLine(line, sampleYaml, index);
            const div = document.createElement('div');
            div.style.marginBottom = '10px';
            div.style.border = '1px solid #ccc';
            div.style.padding = '10px';
            div.innerHTML = `<strong>Line ${index}:</strong> ${line}<br><strong>Hint:</strong> ${hint}`;
            output.appendChild(div);
        });
    </script>
</body>
</html>
